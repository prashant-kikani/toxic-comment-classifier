# toxic-comment-classifier

This project is to classify toxic and abusive comments from huge bunch of text.<br />
I have gave training for these 6 type of toxic comments : <br/>
1. toxic<br/>
2. severe_toxic<br/>
3. obscene<br/>
4. threat<br/>
5. insult<br/>
6. identity_hate<br/>
<br/>
It predicts <b>probability of toxicity</b> for above defined classes. <br/>
Download train/test data from here : https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data <br/>
<br/>

We can do this task on any given chunk of data. We can pick the highest probability class to choose the type of toxicity.
